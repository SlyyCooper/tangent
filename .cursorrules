You are in charge of the 'tangent' python package and the development of projects using it.

### 1. core.py - The Engine
`core.py` implements the main orchestration system for AI agent interactions. Here are its key components:

1. **Class: tangent**
- Main orchestrator that manages conversations with AI agents
- Handles chat completions, function calls, and conversation state
- Key methods:
  - `get_chat_completion`: Sends messages to OpenAI with proper formatting
  - `handle_function_result`: Processes results from tool calls
  - `handle_tool_calls`: Executes functions requested by the AI
  - `run` and `run_and_stream`: Main methods to conduct conversations

The core class supports both streaming and non-streaming modes, manages context variables, and can switch between different agents during a conversation.

### 2. types.py - The Data Models
`types.py` defines the fundamental data structures using Pydantic models:

1. **Class: Agent**
```python
class Agent(BaseModel):
    name: str = "Agent"
    model: str = "gpt-4o"
    instructions: Union[str, Callable[[], str]]
    functions: List[AgentFunction] = []
    tool_choice: str = None
    parallel_tool_calls: bool = True
    embedding_config: Optional[EmbeddingConfig] = None
    _embedding_manager: Optional[EmbeddingManager] = None
```
- Represents an AI agent with its configuration
- Supports embeddings for knowledge base functionality
- Methods for setting up and searching knowledge bases

2. **Class: Response**
```python
class Response(BaseModel):
    messages: List = []
    agent: Optional[Agent] = None
    context_variables: dict = {}
```
- Encapsulates conversation responses
- Tracks messages, active agent, and context

3. **Class: Result**
```python
class Result(BaseModel):
    value: str = ""
    agent: Optional[Agent] = None
    context_variables: dict = {}
```
- Represents the return value from agent functions
- Can include a new agent for switching context
- Maintains conversation context variables

The combination of `core.py` and `types.py` creates a flexible system where:
1. The `Agent` class defines behavior and capabilities
2. The `tangent` class orchestrates conversations
3. `Response` and `Result` classes handle data flow
4. Everything is strongly typed with Pydantic models
5. The system supports both simple conversations and complex multi-agent interactions with tool calls
 

### 1. embeddings.py - Vector Database & Document Management
This is a sophisticated module that handles document processing, embedding generation, and vector search. Here are its key components:

1. **Core Data Classes:**
```python
class DocumentChunk:
    text: str
    metadata: dict
    source_file: str
    chunk_index: int

class Document(BaseModel):
    id: str
    text: str
    metadata: dict = {}
    embedding: Optional[List[float]] = None
```

2. **Configuration Classes:**
```python
class EmbeddingConfig(BaseModel):
    model: str = "text-embedding-3-large"
    chunk_size: int = 500
    chunk_overlap: int = 50
    batch_size: int = 100
    vector_db: Union[QdrantConfig, PineconeConfig, CustomVectorDBConfig]
```
Supports multiple vector databases:
- Qdrant (default)
- Pinecone
- Custom implementations

3. **EmbeddingManager Class:**
Key functionalities:
- Document Processing:
  ```python
  # Supports multiple formats
  - .txt (plain text)
  - .md (markdown)
  - .pdf (PDF documents)
  - .docx (Word documents)
  - .json (JSON with content/metadata)
  ```

- Embedding Creation:
  ```python
  def create_embeddings(self, chunks: List[DocumentChunk]) -> List[Document]:
      # Batches documents
      # Creates embeddings using OpenAI
      # Returns documents with embeddings
  ```

- Vector Search:
  ```python
  def search(self, query: str, top_k: int = 5) -> List[Document]:
      # Creates query embedding
      # Searches vector database
      # Returns similar documents
  ```

4. **DocumentStore Class:**
A high-level interface for document management:
```python
class DocumentStore:
    def __init__(self, documents_path: Optional[str] = None, 
                 auto_process: bool = True):
        # Automatic document processing
        # Vector database setup
```

### 2. knowledge_base.py - Agent Knowledge Integration
This module bridges the embedding system with agents:

1. **Main Function:**
```python
def search_knowledge_base(
    query: str, 
    top_k: int = 5, 
    context_variables: dict = None, 
    agent: Agent = None
) -> Result:
```

Key features:
- Integrates with agent's embedding manager
- Returns formatted search results
- Maintains context through `context_variables`
- Error handling and graceful degradation

2. **Result Format:**
```python
# Success case:
{
    "value": "Document: Title\nContent: Text\nRelevance: Score",
    "context_variables": {
        "last_search_query": query,
        "num_results": count
    }
}

# Error case:
{
    "value": "Error message or 'No results found'",
    "context_variables": original_context
}
```

Together, these modules provide:
1. **Document Processing Pipeline:**
   - Load various document formats
   - Chunk text appropriately
   - Generate embeddings
   - Store in vector database

2. **Search Capabilities:**
   - Semantic search using embeddings
   - Multiple vector database options
   - Configurable result count

3. **Agent Integration:**
   - Agents can have their own knowledge bases
   - Search results format into conversation context
   - Error handling and state management

This system allows agents to:
- Access document-based knowledge
- Perform semantic search
- Maintain context across conversations
- Handle multiple document formats
- Scale with different vector database backends

The combination of embeddings and knowledge base creates a powerful system for agents to access and utilize document-based knowledge in their conversations.

# Project Structure

<tree_structure>
.
├── .cursorignore
├── .cursorrules
├── .env
├── .gitattributes
├── .gitignore
├── README.md
├── config
│   ├── agents
│   │   ├── gateway_agent
│   │   │   ├── __init__.py
│   │   │   ├── agents.py
│   │   │   ├── evals.py
│   │   │   ├── evals_util.py
│   ├── tools
│   │   ├── __init__.py
│   │   ├── applescript_tool.py
│   │   ├── database_tool.py
│   │   ├── terminal_tool.py
├── docs
│   ├── creating_agents.md
│   ├── embeddings.md
│   ├── function_calls.md
│   ├── triage_template.md
├── document_storage
│   ├── 0e4f6976-1ce1-46f9-9ae0-64ab5a27bbc0.json
│   ├── a5048f2c-7399-47c6-b721-7e93affa9648.json
├── examples
│   ├── embedding_agent.py
│   ├── websearch_agent.py
├── main.py
├── my-docs
│   ├── tangent_python_library.md
├── pyproject.toml
├── setup.cfg
├── tangent
│   ├── __init__.py
│   ├── core.py
│   ├── embeddings.py
│   ├── repl
│   │   ├── __init__.py
│   │   ├── repl.py
│   ├── tools
│   │   ├── knowledge_base.py
│   ├── types.py
│   ├── util.py
├── test_documents
│   ├── ai_history.json
│   ├── articles
│   │   ├── deep_learning.md
│   │   ├── quantum_computing.docx
│   │   ├── robotics.txt
</tree_structure>

**IMPORTANT**
1. The Gateway Agent should be both:
 - A direct executor (using tools directly for simple tasks)
 - An orchestrator (delegating to specialized agents for complex tasks)
2. All tool functions should return a Result object.
3. The tool functions are defined in the tool file and then imported and wrapped by the agent.

# **tangent Python Library Documentation**

### **Overview**

The `core.py` module implements the main tangent system, which manages agent-based chat completions and function execution. It provides a flexible framework for handling conversational AI agents with support for tool calls, context management, and streaming responses.

### **Class: tangent**

#### **Description**

The `tangent` class is the primary interface for managing AI agent interactions. It handles chat completions, function calls, and maintains conversation state across multiple turns.

#### **Initialization**

```python
def __init__(self, client=None):
    if not client:
        client = OpenAI()
    self.client = client
```

- The `tangent` class can be initialized with an optional OpenAI client.  
- If no client is provided, it creates a default OpenAI client instance.

# tangent Python Library Documentation

Welcome to the **tangent** Python library documentation. **tangent** is a high-level framework designed to facilitate conversational interactions and tool/agent orchestration using OpenAI-based Large Language Models (LLMs). This documentation provides a comprehensive guide on how to install, configure, and use **tangent**, with detailed API references for each module, class, and function.

---

## Table of Contents

1. [Overview](#overview)  
2. [Installation](#installation)  
3. [Quick Start Example](#quick-start-example)  
4. [Package Structure](#package-structure)  
5. [API Reference](#api-reference)  
   - [1. `tangent.__init__`](#1-tangentinitpy)
   - [2. `tangent.core`](#2-tangentcorepy)
   - [3. `tangent.repl`](#3-tangentrepl)
     - [3.1 `tangent.repl.__init__`](#31-tangentreplinitpy)
     - [3.2 `tangent.repl.repl`](#32-tangentreplreplpy)
   - [4. `tangent.types`](#4-tangenttypespy)
   - [5. `tangent.util`](#5-tangentutilpy)
6. [Additional Notes](#additional-notes)

---

## Overview

**tangent** provides:

- A flexible `Agent` abstraction for describing how an AI assistant interacts with a user or environment.
- Utility methods to handle **tool calls** (function calls) automatically invoked by the AI.
- Support for streaming responses vs. one-shot completions.
- A simple REPL (Read-Eval-Print Loop) interface for interactive experimentation.
- Integration with [OpenAI Python bindings](https://github.com/openai/openai-python).

At its core, **tangent** handles:
1. Sending chat history (messages) along with instructions to the LLM (OpenAI).
2. Optionally handling function calls (tools) that the LLM requests to execute.
3. Managing context variables across multiple chat turns.
4. Streaming partial responses for interactive experiences or collecting them in a single response.

---

## Installation

Because this documentation is based solely on the code provided (and does not specify a distribution method), you can install **tangent** in one of two ways:

1. **Cloning the Repository**  
   Clone the repository containing the `tangent/` package into your local environment. Then install the package via:
   ```bash
   pip install -e .
   ```
   or ensure it is in your `PYTHONPATH`.

2. **Manual Usage**  
   Copy the `tangent/` folder into your project and reference it directly.

Make sure you also install the dependencies:

- `openai` (Required)  
- `pydantic` (Required)  

For example:
```bash
pip install openai pydantic
```

---

## Quick Start Example

Below is a simple example showing how to use the **tangent** library to run an interactive loop with an `Agent`:

```python
from tangent import Agent, tangent
from tangent.types import Response

# 1. Define a simple function (tool) used by the agent
def greet(name: str):
    """
    Greet a user by name.
    """
    return f"Hello, {name}!"

# 2. Define an agent with instructions and a function
my_agent = Agent(
    name="GreeterAgent",
    model="gpt-4o",
    instructions="You are a friendly greeter. Use the greet function to greet users by name.",
    functions=[greet],
)

# 3. Initialize the tangent runner
client = tangent()

# 4. Provide some initial chat messages
messages = [
    {"role": "user", "content": "Hi there, I'm John!"}
]

# 5. Run a single pass
response: Response = client.run(
    agent=my_agent,
    messages=messages,
    context_variables={},
    stream=False,
    debug=True
)

# 6. Print out the final messages
for msg in response.messages:
    print(f"{msg.get('sender', msg.get('role','assistant'))}: {msg['content']}")
```

---

## Package Structure

```
tangent
├── __init__.py
├── core.py
├── repl
│   ├── __init__.py
│   └── repl.py
├── types.py
└── util.py
```

### Overview of Each Module

- **`tangent/__init__.py`**  
  Exports high-level classes and methods such as:
  - `tangent.tangent`
  - `tangent.Agent`
  - `tangent.Response`

- **`tangent/core.py`**  
  Contains the main `tangent` class, which orchestrates:
  - Chat completions
  - Tool calls
  - Streaming vs. non-streaming run methods

- **`tangent/repl/__init__.py`** and **`tangent/repl/repl.py`**  
  Provides a CLI demonstration and helper methods to process streaming responses, as well as a simple REPL loop.

- **`tangent/types.py`**  
  Defines the Pydantic data models:
  - `Agent`
  - `Response`
  - `Result`
  Also re-exports or references classes from `openai.types.chat`.

- **`tangent/util.py`**  
  Provides internal utility functions (debug printing, chunk merging, function-to-JSON conversion, etc.).

---

## API Reference

### 1. `tangent/__init__.py`

```python
from .core import tangent
from .types import Agent, Response

__all__ = ["tangent", "Agent", "Response"]
```

- **`tangent`**  
  A reference to the `tangent.tangent` class in `core.py`. This is the main entry point for running chat completions and orchestrating tools.

- **`Agent`**  
  A Pydantic model from `tangent.types.Agent`.

- **`Response`**  
  A Pydantic model from `tangent.types.Response`.

---

### 2. `tangent/core.py`

This file contains the primary `tangent` class and associated methods used for orchestrating chat flows with or without streaming. Below is a breakdown of the important contents.

#### 2.1 Class: `tangent`

```python
class tangent:
    def __init__(self, client=None):
        ...
```

**Description**  
The main orchestrator class. Manages conversation flow, obtains completions from OpenAI, interprets tool calls, and can run in a loop or stream results incrementally.

**Parameters**  
- `client`: *(optional)* An instance of the OpenAI client. If not provided, `tangent` instantiates a default `OpenAI()` client.

---

#### 2.2 Method: `get_chat_completion(...)`

```python
def get_chat_completion(
    self,
    agent: Agent,
    history: List,
    context_variables: dict,
    model_override: str,
    stream: bool,
    debug: bool,
) -> ChatCompletionMessage:
    ...
```

**Description**  
Generates a chat completion from the current conversation `history` using the provided `agent` and `context_variables`. Internally, it calls `self.client.chat.completions.create` with the necessary parameters. It also modifies the tool function parameters to remove internal context from the final LLM prompt.

**Parameters**  
- `agent` (`Agent`): The agent object containing instructions, model, and possible tool functions.  
- `history` (`List`): A list of previous messages in the conversation. Each element is typically a dict with keys like `"role"` and `"content"`.  
- `context_variables` (`dict`): Additional data for the agent (a dictionary of context variables).  
- `model_override` (`str`): Optional model name to override the agent's default model.  
- `stream` (`bool`): Whether to enable streaming from the LLM.  
- `debug` (`bool`): If `True`, prints debug messages via `debug_print`.

**Returns**  
- `ChatCompletionMessage`: The raw completion message (OpenAI-specific type).

---

#### 2.3 Method: `handle_function_result(...)`

```python
def handle_function_result(self, result, debug) -> Result:
    ...
```

**Description**  
Interprets the return value from a tool call. This can be:
- A `Result` object (already well-formed)
- An `Agent` (wrapped into a `Result` object)
- A plain string, dictionary, or something else (converted to a `Result` with `str(result)`).

**Parameters**  
- `result`: The raw result from calling an agent function.  
- `debug` (`bool`): If `True`, logs debug output.  

**Returns**  
- `Result`: A standardized `Result` object containing `.value`, optional `.agent`, and `.context_variables`.

---

#### 2.4 Method: `handle_tool_calls(...)`

```python
def handle_tool_calls(
    self,
    tool_calls: List[ChatCompletionMessageToolCall],
    functions: List[AgentFunction],
    context_variables: dict,
    debug: bool,
) -> Response:
    ...
```

**Description**  
Executes one or more tool calls (functions). Each `tool_call` references the function name, arguments, and additional metadata. The function looks up the actual Python function by name in `functions`, executes it, and collects results as messages to be appended to the overall conversation.

**Parameters**  
- `tool_calls` (`List[ChatCompletionMessageToolCall]`): The list of tool calls requested by the LLM.  
- `functions` (`List[AgentFunction]`): The list of actual agent functions the agent can call.  
- `context_variables` (`dict`): The shared context dictionary.  
- `debug` (`bool`): Whether to enable debug printing.

**Returns**  
- `Response`: A `Response` object containing any additional messages (tool results) plus updates to `context_variables` or the active `agent`.

---

#### 2.5 Method: `run_and_stream(...)`

```python
def run_and_stream(
    self,
    agent: Agent,
    messages: List,
    context_variables: dict = {},
    model_override: str = None,
    debug: bool = False,
    max_turns: int = float("inf"),
    execute_tools: bool = True,
):
    ...
```

**Description**  
Initiates a conversation loop with an `agent` and yields tokens/chunks in real-time. This method is a **generator** that yields partial response data as the LLM streams responses.  

At each turn:
1. Sends the conversation history to the model.
2. Streams the response chunk-by-chunk.
3. If any tools are requested, it executes them and appends their outputs to the history.
4. Updates `context_variables` and possibly switches `agent` if the returned `Result` includes a new agent.

**Parameters**  
- `agent` (`Agent`): The initial conversation agent.  
- `messages` (`List`): The conversation history, usually a list of dicts.  
- `context_variables` (`dict`, optional): Shared context data.  
- `model_override` (`str`, optional): Overrides the agent’s model if set.  
- `debug` (`bool`, optional): Enables debug logs if `True`.  
- `max_turns` (`int`, optional): Maximum number of back-and-forth cycles.  
- `execute_tools` (`bool`, optional): Whether to allow function calls.

**Yields**  
- Partial JSON-like dictionaries representing incremental response data, and eventually a final dictionary containing the full `Response`.

---

#### 2.6 Method: `run(...)`

```python
def run(
    self,
    agent: Agent,
    messages: List,
    context_variables: dict = {},
    model_override: str = None,
    stream: bool = False,
    debug: bool = False,
    max_turns: int = float("inf"),
    execute_tools: bool = True,
) -> Response:
    ...
```

**Description**  
A high-level method to manage conversation flow with optional streaming.  
- If `stream` is `True`, it delegates to `run_and_stream`, returning a generator.  
- If `stream` is `False`, it runs the conversation to completion synchronously and returns a final `Response`.

**Parameters**  
- `agent` (`Agent`): The conversation agent.  
- `messages` (`List`): Conversation history.  
- `context_variables` (`dict`, optional): Shared context dictionary.  
- `model_override` (`str`, optional): Overriding model name.  
- `stream` (`bool`, optional): If `True`, streaming mode.  
- `debug` (`bool`, optional): Enables debug logging.  
- `max_turns` (`int`, optional): Maximum loop iterations.  
- `execute_tools` (`bool`, optional): If `True`, executes requested tool calls.

**Returns**  
- `Response`: Contains the conversation messages from the starting turn to the final turn, the last active `agent`, and any updated context variables.

---

### 3. `tangent/repl`

#### 3.1 `tangent/repl/__init__.py`

```python
from .repl import run_tangent_loop
```

Exports the `run_tangent_loop` function, which is a handy method for an interactive CLI demo.

#### 3.2 `tangent/repl/repl.py`

Key functionalities:

1. **`process_and_print_streaming_response(response)`**  
   A helper function to consume a streaming generator from `tangent.run(..., stream=True)` and print out tokens as they arrive. It also detects the end of each chunk.

   ```python
   def process_and_print_streaming_response(response):
       ...
   ```

2. **`pretty_print_messages(messages)`**  
   A convenience function to format and print conversation messages, including any tool calls.

   ```python
   def pretty_print_messages(messages) -> None:
       ...
   ```

3. **`run_tangent_loop(starting_agent, context_variables=None, stream=False, debug=False) -> None`**  
   Runs an interactive command-line loop:
   - Prompts user input
   - Appends it to a conversation history
   - Calls `tangent.run` with the specified `agent`
   - Prints the returned messages or streams them
   - Updates the conversation state and repeats until termination

   ```python
   def run_tangent_loop(
       starting_agent, context_variables=None, stream=False, debug=False
   ) -> None:
       ...
   ```

Usage example:

```bash
python -m tangent.repl  # if an entry point is defined
```

Then follow the prompts in the terminal to interact with your agent.

---

### 4. `tangent/types.py`

Defines the core data models, built with `pydantic`. Also references `openai.types.chat` internally.

```python
from openai.types.chat import ChatCompletionMessage
from openai.types.chat.chat_completion_message_tool_call import (
    ChatCompletionMessageToolCall,
    Function,
)
```

- **`AgentFunction`**  
  ```python
  AgentFunction = Callable[[], Union[str, "Agent", dict]]
  ```
  A convenient type alias for agent-related tool functions.

---

#### 4.1 Class: `Agent`

```python
class Agent(BaseModel):
    name: str = "Agent"
    model: str = "gpt-4o"
    instructions: Union[str, Callable[[], str]] = "You are a helpful agent."
    functions: List[AgentFunction] = []
    tool_choice: str = None
    parallel_tool_calls: bool = True
```

**Description**  
Represents an AI agent configuration. This includes:
- A `name` (default `"Agent"`).
- The `model` name used for the LLM (default `"gpt-4o"`).
- `instructions`, a string or callable that returns a string (the "system" message or prompt).
- A list of `functions` (Python callables) that can be invoked by the LLM as "tools".
- `tool_choice` (string) to specify a strategy or preference for tool usage (optional).
- `parallel_tool_calls` (bool) indicating if multiple tool calls can be made in parallel (default `True`).

---

#### 4.2 Class: `Response`

```python
class Response(BaseModel):
    messages: List = []
    agent: Optional[Agent] = None
    context_variables: dict = {}
```

**Description**  
Encapsulates the final or intermediate result of a conversation step (or multiple steps).

**Fields**  
- `messages` (`List`): The updated conversation history messages.  
- `agent` (`Optional[Agent]`): The last active agent after processing.  
- `context_variables` (`dict`): The updated context variables after function calls or LLM responses.

---

#### 4.3 Class: `Result`

```python
class Result(BaseModel):
    """
    Encapsulates the possible return values for an agent function.

    Attributes:
        value (str): The result value as a string.
        agent (Agent): The agent instance, if applicable.
        context_variables (dict): A dictionary of context variables.
    """

    value: str = ""
    agent: Optional[Agent] = None
    context_variables: dict = {}
```

**Description**  
Provides a flexible structure for any function return value. The `.value` is the main string output, `.agent` can replace the active agent, and `.context_variables` can update or override the conversation state.

---

### 5. `tangent/util.py`

Contains various helper functions for internal usage.

1. **`debug_print(debug: bool, *args: str) -> None`**  
   Prints debugging information if `debug=True`. Prepends a timestamp and uses colored formatting.

2. **`merge_fields(target, source)`**  
   Recursively merges values from `source` dict into `target` dict (for partial chunk updates).

3. **`merge_chunk(final_response: dict, delta: dict) -> None`**  
   Merges a streaming delta chunk into a final response dictionary. Manages tool call indexing as well.

4. **`function_to_json(func) -> dict`**  
   Converts a Python function’s signature and docstring to a JSON-compatible dictionary structure. This is used to prepare the function schema for the LLM so it knows how to call tools.

   ```python
   def function_to_json(func) -> dict:
       """
       Converts a Python function into a JSON-serializable dictionary
       that describes the function's signature, including its name,
       description, and parameters.
       """
       ...
   ```

---
## PostgreSQL Database Used 
Let me help you find your PostgreSQL connection details:

```bash
/opt/homebrew/opt/postgresql@16/bin/psql -c "SHOW port;" postgres | cat
```

```bash
/opt/homebrew/opt/postgresql@16/bin/psql -c "SHOW listen_addresses;" postgres | cat
```


Here are your PostgreSQL connection details:

1. Connection URL format:
```
postgresql://tan@localhost:5432/postgres
```

2. Individual connection parameters:
- Host: `localhost` (or `127.0.0.1`)
- Port: `5432`
- User: `tan`
- Default database: `postgres`

You can connect to it in several ways:

1. **Using psql command line**:
```bash
/opt/homebrew/opt/postgresql@16/bin/psql -h localhost -p 5432 -U tan -d postgres
```

2. **Using a connection string**:
```
postgresql://tan@localhost:5432/postgres
```

3. **Using individual parameters**:
```python
# Python example with psycopg2
{
    "host": "localhost",
    "port": "5432",
    "user": "tan",
    "dbname": "postgres"
}
```

---

## Additional Notes

- **Context Variable Handling**: If you wish to pass additional data from one conversation turn to the next (e.g., user preferences, session data, etc.), you can store them in `context_variables`. **tangent** automatically provides them to your tools that declare `context_variables` in their function signature.
- **Streaming vs. Non-streaming**: When setting `stream=True` in the `run(...)` method, **tangent** yields partial tokens as they arrive from the LLM. You can consume these tokens one at a time to build interactive UIs or real-time displays.
- **Switching Agents**: A tool can return a `Result` with an `.agent` field, effectively switching to a new agent mid-conversation.
- **Debugging**: Set `debug=True` in `run(...)` or `run_and_stream(...)` to see debug logs from **tangent**, including messages sent to the LLM, tool calls, and partial tokens.

## Conclusion

**tangent** is a straightforward yet flexible library for orchestrating multi-step conversations with LLMs, especially when function calls (tools) are involved. It provides:
- Clear abstractions for an `Agent`’s instructions and available functions.
- Automatic management of tool calls requested by the model.
- Streaming or one-shot conversation flows.
- Easy integration into a CLI environment (via `tangent/repl`).